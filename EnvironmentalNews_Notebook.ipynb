{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhanlp import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import *\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import CoherenceModel\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "path = 'environmentalnews.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# get the content column of data\n",
    "content = data['content']\n",
    "\n",
    "# remove blanks\n",
    "content_tight = [cont.replace(' ', '') for cont in content]\n",
    "\n",
    "# redo segment by HanLP and change strings to lists of words\n",
    "content_list = []\n",
    "for cont in content_tight:\n",
    "    content_list.append(HanLP.segment(cont).toString().split(', '))\n",
    "\n",
    "# remove '[' in first element\n",
    "content_clean  = [[c.replace('[','') for c in cont] for cont in content_list]\n",
    "\n",
    "# only select nouns\n",
    "noun_list = [[word.split('/')[0] for word in cont if '/n' in word \n",
    "              and '/nr' not in word and '/ns' not in word and '/nx' not in word] for cont in content_clean]\n",
    "\n",
    "# divide the dataset into train and test\n",
    "train, test = train_test_split(noun_list, test_size = 0.2, random_state = 100)\n",
    "\n",
    "# Dictionary and Corpus on train set\n",
    "dict_train = corpora.Dictionary(train)\n",
    "corpus_train = [dict_train.doc2bow(t) for t in train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 topics\n",
    "n_topic = 10\n",
    "topic_10 = models.LdaModel(corpus_train, id2word = dict_train, num_topics = n_topic)\n",
    "# 20 topics\n",
    "n_topic = 20\n",
    "topic_20 = models.LdaModel(corpus_train, id2word = dict_train, num_topics = n_topic)\n",
    "# 30 topics\n",
    "n_topic = 30\n",
    "topic_30 = models.LdaModel(corpus_train, id2word = dict_train, num_topics = n_topic)\n",
    "# 50 topics\n",
    "n_topic = 50\n",
    "topic_50 = models.LdaModel(corpus_train, id2word = dict_train, num_topics = n_topic)\n",
    "# 75 topics\n",
    "n_topic = 75\n",
    "topic_75 = models.LdaModel(corpus_train, id2word = dict_train, num_topics = n_topic)\n",
    "# 100 topics\n",
    "n_topic = 100\n",
    "topic_100 = models.LdaModel(corpus_train, id2word = dict_train, num_topics = n_topic)\n",
    "# 200 topics\n",
    "n_topic = 200\n",
    "topic_200 = models.LdaModel(corpus_train, id2word = dict_train, num_topics = n_topic)\n",
    "# 300 topics\n",
    "n_topic = 300\n",
    "topic_300 = models.LdaModel(corpus_train, id2word = dict_train, num_topics = n_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coherance score of different models\n",
    "coherence_model_lda10 = CoherenceModel(model=topic_10, texts=test, dictionary=dict_train, coherence='c_v')\n",
    "coherence_model_lda20 = CoherenceModel(model=topic_20, texts=test, dictionary=dict_train, coherence='c_v')\n",
    "coherence_model_lda30 = CoherenceModel(model=topic_30, texts=test, dictionary=dict_train, coherence='c_v')\n",
    "coherence_model_lda50 = CoherenceModel(model=topic_50, texts=test, dictionary=dict_train, coherence='c_v')\n",
    "coherence_model_lda75 = CoherenceModel(model=topic_75, texts=test, dictionary=dict_train, coherence='c_v')\n",
    "coherence_model_lda100 = CoherenceModel(model=topic_100, texts=test, dictionary=dict_train, coherence='c_v')\n",
    "coherence_model_lda200 = CoherenceModel(model=topic_200, texts=test, dictionary=dict_train, coherence='c_v')\n",
    "coherence_model_lda300 = CoherenceModel(model=topic_300, texts=test, dictionary=dict_train, coherence='c_v')\n",
    "coherence_lda10 = coherence_model_lda10.get_coherence()\n",
    "coherence_lda20 = coherence_model_lda20.get_coherence()\n",
    "coherence_lda30 = coherence_model_lda30.get_coherence()\n",
    "coherence_lda50 = coherence_model_lda50.get_coherence()\n",
    "coherence_lda75 = coherence_model_lda75.get_coherence()\n",
    "coherence_lda100 = coherence_model_lda100.get_coherence()\n",
    "coherence_lda200 = coherence_model_lda200.get_coherence()\n",
    "coherence_lda300 = coherence_model_lda300.get_coherence()\n",
    "coherence_lda10_detail = coherence_model_lda10.get_coherence_per_topic()\n",
    "coherence_lda20_detail = coherence_model_lda20.get_coherence_per_topic()\n",
    "coherence_lda30_detail = coherence_model_lda30.get_coherence_per_topic()\n",
    "coherence_lda50_detail = coherence_model_lda50.get_coherence_per_topic()\n",
    "coherence_lda75_detail = coherence_model_lda75.get_coherence_per_topic()\n",
    "coherence_lda100_detail = coherence_model_lda100.get_coherence_per_topic()\n",
    "coherence_lda200_detail = coherence_model_lda200.get_coherence_per_topic()\n",
    "coherence_lda300_detail = coherence_model_lda300.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trend visualization preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to create topic vector for every model\n",
    "def topicvec(n_topic, model):\n",
    "    result = []\n",
    "    for c in corpus_train:\n",
    "        temp = [0] * n_topic\n",
    "        for array in model[c]:\n",
    "            temp[array[0]] = array[1]\n",
    "        result.append(temp)\n",
    "    return result\n",
    "\n",
    "# create topic vector for each model\n",
    "result10 = topicvec(10, topic_10)\n",
    "result20 = topicvec(20, topic_20)\n",
    "result30 = topicvec(30, topic_30)\n",
    "result50 = topicvec(50, topic_50)\n",
    "result75 = topicvec(75, topic_75)\n",
    "result100 = topicvec(100, topic_100)\n",
    "result200 = topicvec(200, topic_200)\n",
    "result300 = topicvec(300, topic_300)\n",
    "\n",
    "# get the length of every document\n",
    "content_length = np.array([len(cont) for cont in content_list])\n",
    "\n",
    "# preparation\n",
    "train_length, test_length = train_test_split(content_length, test_size = 0.2, random_state = 100)\n",
    "train_date, test_date = train_test_split(data['date'], test_size = 0.2, random_state = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save everything for future use"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# topic vectors\n",
    "with open('result10', 'wb') as f:\n",
    "    pickle.dump(result10, f)\n",
    "with open('result20', 'wb') as f:\n",
    "    pickle.dump(result20, f)\n",
    "with open('result30', 'wb') as f:\n",
    "    pickle.dump(result30, f)\n",
    "with open('result50', 'wb') as f:\n",
    "    pickle.dump(result50, f)\n",
    "with open('result75', 'wb') as f:\n",
    "    pickle.dump(result75, f)\n",
    "with open('result100', 'wb') as f:\n",
    "    pickle.dump(result100, f)\n",
    "with open('result200', 'wb') as f:\n",
    "    pickle.dump(result200, f)\n",
    "with open('result300', 'wb') as f:\n",
    "    pickle.dump(result300, f)\n",
    "\n",
    "# train length\n",
    "with open('TrainLength', 'wb') as f:\n",
    "    pickle.dump(train_length, f)\n",
    "\n",
    "# train date\n",
    "with open('TrainDate', 'wb') as f:\n",
    "    pickle.dump(train_date, f)\n",
    "\n",
    "# models\n",
    "topic_10.save('lda10')\n",
    "topic_20.save('lda20')\n",
    "topic_30.save('lda30')\n",
    "topic_50.save('lda50')\n",
    "topic_75.save('lda75')\n",
    "topic_100.save('lda100')\n",
    "topic_200.save('lda200')\n",
    "topic_300.save('lda300')\n",
    "\n",
    "# coference\n",
    "with open('Cohe10', 'wb') as f:\n",
    "    pickle.dump(coherence_lda10_detail, f)\n",
    "with open('Cohe20', 'wb') as f:\n",
    "    pickle.dump(coherence_lda20_detail, f)\n",
    "with open('Cohe30', 'wb') as f:\n",
    "    pickle.dump(coherence_lda30_detail, f)\n",
    "with open('Cohe50', 'wb') as f:\n",
    "    pickle.dump(coherence_lda50_detail, f)\n",
    "with open('Cohe75', 'wb') as f:\n",
    "    pickle.dump(coherence_lda75_detail, f)\n",
    "with open('Cohe100', 'wb') as f:\n",
    "    pickle.dump(coherence_lda100_detail, f)\n",
    "with open('Cohe200', 'wb') as f:\n",
    "    pickle.dump(coherence_lda200_detail, f)\n",
    "with open('Cohe300', 'wb') as f:\n",
    "    pickle.dump(coherence_lda300_detail, f)\n",
    "    \n",
    "# dictionary and corpus\n",
    "dict_train.save('DictT.dict')\n",
    "corpora.MmCorpus.serialize('CorpusT.mm', corpus_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load in pre-saved lists and models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# topic vectors\n",
    "with open('result10', 'rb') as f:\n",
    "    result10 = pickle.load(f)\n",
    "with open('result20', 'rb') as f:\n",
    "    result20 = pickle.load(f)\n",
    "with open('result30', 'rb') as f:\n",
    "    result30 = pickle.load(f)\n",
    "with open('result50', 'rb') as f:\n",
    "    result50 = pickle.load(f)\n",
    "with open('result75', 'rb') as f:\n",
    "    result75 = pickle.load(f)\n",
    "with open('result100', 'rb') as f:\n",
    "    result100 = pickle.load(f)\n",
    "with open('result200', 'rb') as f:\n",
    "    result200 = pickle.load(f)\n",
    "with open('result300', 'rb') as f:\n",
    "    result300 = pickle.load(f)\n",
    "\n",
    "# train length\n",
    "with open('TrainLength', 'rb') as f:\n",
    "    train_length = pickle.load(f)\n",
    "\n",
    "# train date\n",
    "with open('TrainDate', 'rb') as f:\n",
    "    train_date = pickle.load(f)\n",
    "\n",
    "# models\n",
    "topic_10 = models.LdaModel.load('lda10')\n",
    "topic_20 = models.LdaModel.load('lda20')\n",
    "topic_30 = models.LdaModel.load('lda30')\n",
    "topic_50 = models.LdaModel.load('lda50')\n",
    "topic_75 = models.LdaModel.load('lda75')\n",
    "topic_100 = models.LdaModel.load('lda100')\n",
    "topic_200 = models.LdaModel.load('lda200')\n",
    "topic_300 = models.LdaModel.load('lda300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load in coherence scores, dictionaries and corpus"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# coference\n",
    "with open('Cohe10', 'rb') as f:\n",
    "    coherence_lda10_detail = pickle.load(f)\n",
    "with open('Cohe20', 'rb') as f:\n",
    "    coherence_lda20_detail = pickle.load(f)\n",
    "with open('Cohe30', 'rb') as f:\n",
    "    coherence_lda30_detail = pickle.load(f)\n",
    "with open('Cohe50', 'rb') as f:\n",
    "    coherence_lda50_detail = pickle.load(f)\n",
    "with open('Cohe75', 'rb') as f:\n",
    "    coherence_lda75_detail = pickle.load(f)\n",
    "with open('Cohe100', 'rb') as f:\n",
    "    coherence_lda100_detail = pickle.load(f)\n",
    "with open('Cohe200', 'rb') as f:\n",
    "    coherence_lda200_detail = pickle.load(f)\n",
    "with open('Cohe300', 'rb') as f:\n",
    "    coherence_lda300_detail = pickle.load(f)\n",
    "\n",
    "# dictionary and corpus\n",
    "dict_train = corpora.Dictionary.load('DictT.dict')\n",
    "corpus_train = corpora.MmCorpus('CorpusT.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Find higher score topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find which topic is good enough to pass the standard of 0.45 or 0.5\n",
    "good10 = [coherence_lda10_detail.index(cld) for cld in coherence_lda10_detail if cld > 0.5]\n",
    "good20 = [coherence_lda20_detail.index(cld) for cld in coherence_lda20_detail if cld > 0.5]\n",
    "good30 = [coherence_lda30_detail.index(cld) for cld in coherence_lda30_detail if cld > 0.5]\n",
    "good50 = [coherence_lda50_detail.index(cld) for cld in coherence_lda50_detail if cld > 0.5]\n",
    "good75 = [coherence_lda75_detail.index(cld) for cld in coherence_lda75_detail if cld > 0.5]\n",
    "good100 = [coherence_lda100_detail.index(cld) for cld in coherence_lda100_detail if cld > 0.55]\n",
    "good200 = [coherence_lda200_detail.index(cld) for cld in coherence_lda200_detail if cld > 0.55]\n",
    "good300 = [coherence_lda300_detail.index(cld) for cld in coherence_lda300_detail if cld > 0.55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to draw topic-time distribution\n",
    "def topicplot(n, topic_vector):\n",
    "    # extract each document's topic proportion for topic no.n\n",
    "    topicn = [t[n] for t in topic_vector]\n",
    "    # calculate the score of topic n of each document by mutiply the length with topic proportion\n",
    "    topicn = np.array(topicn)\n",
    "    score = topicn * train_length\n",
    "    # create a pd dataframe of date and score\n",
    "    doc_datescore = pd.DataFrame({'date':train_date,'score':score})\n",
    "    # keep only year and month\n",
    "    doc_datescore['date'] = [date[:7] for date in doc_datescore['date']]\n",
    "    # group by months and sum scores up\n",
    "    datescore = doc_datescore.groupby(['date'], as_index = False).sum()\n",
    "    x = [datetime.strptime(d, '%Y-%m') for d in datescore['date']]\n",
    "    y = datescore['score']\n",
    "    plt.plot(x, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google translation\n",
    "translate = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot topic trend for good topics of topic_10\n",
    "for i in good10:\n",
    "    print(topic_10.print_topics(num_topics = 10, num_words = 10)[i])\n",
    "    string = str(list(topic_10.print_topics(num_topics = 10, num_words = 10)[i]))\n",
    "    result = translate.translate(string)\n",
    "    print(result.text)\n",
    "    topicplot(i, result10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot topic trend for good topics of topic_20\n",
    "for i in good20:\n",
    "    print(topic_20.print_topics(num_topics = 20, num_words = 10)[i])\n",
    "    string = str(list(topic_20.print_topics(num_topics = 20, num_words = 10)[i]))\n",
    "    result = translate.translate(string)\n",
    "    print(result.text)\n",
    "    topicplot(i, result20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot topic trend for good topics of topic_30\n",
    "for i in good30:\n",
    "    print(topic_30.print_topics(num_topics = 30, num_words = 10)[i])\n",
    "    string = str(list(topic_30.print_topics(num_topics = 30, num_words = 10)[i]))\n",
    "    result = translate.translate(string)\n",
    "    print(result.text)\n",
    "    topicplot(i, result30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot topic trend for good topics of topic_50\n",
    "for i in good40:\n",
    "    print(topic_50.print_topics(num_topics = 50, num_words = 10)[i])\n",
    "    string = str(list(topic_50.print_topics(num_topics = 50, num_words = 10)[i]))\n",
    "    result = translate.translate(string)\n",
    "    print(result.text)\n",
    "    topicplot(i, result50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot topic trend for good topics of topic_75\n",
    "for i in good75\n",
    "    print(topic_75.print_topics(num_topics = 75, num_words = 10)[i])\n",
    "    string = str(list(topic_75.print_topics(num_topics = 75, num_words = 10)[i]))\n",
    "    result = translate.translate(string)\n",
    "    print(result.text)\n",
    "    topicplot(i, result75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot topic trend for good topics of topic_100\n",
    "for i in good100:\n",
    "    print(topic_100.print_topics(num_topics = 100, num_words = 10)[i])\n",
    "    string = str(list(topic_100.print_topics(num_topics = 100, num_words = 10)[i]))\n",
    "    result = translate.translate(string)\n",
    "    print(result.text)\n",
    "    topicplot(i, result100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot topic trend for good topics of topic_200\n",
    "for i in good200:\n",
    "    print(topic_200.print_topics(num_topics = 200, num_words = 10)[i])\n",
    "    string = str(list(topic_200.print_topics(num_topics = 200, num_words = 10)[i]))\n",
    "    result = translate.translate(string)\n",
    "    print(result.text)\n",
    "    topicplot(i, result200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot topic trend for good topics of topic_300\n",
    "for i in good300:\n",
    "    print(topic_300.print_topics(num_topics = 300, num_words = 10)[i])\n",
    "    string = str(list(topic_300.print_topics(num_topics = 300, num_words = 10)[i]))\n",
    "    result = translate.translate(string)\n",
    "    print(result.text)\n",
    "    topicplot(i, result300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
